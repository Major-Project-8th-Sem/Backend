{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_c1x1qwZvvw"
      },
      "outputs": [],
      "source": [
        "# All  import statements needed for the notebook\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.metrics import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IPfkYuBZ4HN",
        "outputId": "2687bced-2a28-4015-f219-a87de2e59a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MrIf225aITv",
        "outputId": "e8122851-7119-431d-b68c-e988f50871e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/balancedhalf_data.csv')\n",
        "data.drop(data.columns[0] , inplace=True , axis=1)\n",
        "unknown = ['SSH-Patator','DoS slowloris','DoS Slowhttptest','Bot','Infiltration','Heartbleed']\n",
        "att = data.loc[(data['Label'].isin(unknown))]\n",
        "data.drop(att.index,axis=0 , inplace=True, errors='ignore')"
      ],
      "metadata": {
        "id": "hL3YlMdZaS7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PreProcessing"
      ],
      "metadata": {
        "id": "BPmJxRB4ahHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries for normalizing data\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# selecting numeric attributes columns from data\n",
        "numeric_col = data.select_dtypes(include='number').columns\n",
        "# using standard scaler for normalizing\n",
        "std_scaler = MinMaxScaler()\n",
        "def normalization(df,att,col):\n",
        "  for i in col:\n",
        "    arr = df[i]\n",
        "    arr = np.array(arr)\n",
        "    x = np.array(att[i])\n",
        "    df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
        "    #To use the same scaler which was used in preprocessing the train data \n",
        "    att[i] = std_scaler.transform(x.reshape(len(x),1))\n",
        "  return df,att\n",
        "# calling the normalization() function\n",
        "data , att = normalization(data.copy(),att.copy(),numeric_col)\n",
        "att.Label = 'unknown'\n",
        "X = att.drop('Label' , axis=1)\n",
        "X = X.to_numpy().reshape(-1, 83,1)\n",
        "\n",
        "y = att.Label\n",
        "X_train = data.drop('Label' , axis=1)\n",
        "X_train = X_train.to_numpy().reshape(-1, 83,1)\n",
        "\n",
        "y_train = data.Label"
      ],
      "metadata": {
        "id": "XW-dLBpGaeer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Model and Predict**\n"
      ],
      "metadata": {
        "id": "KAn3vomoazuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model('/content/drive/MyDrive/models/model_split_softmax_cnn_model.hdf5')"
      ],
      "metadata": {
        "id": "MhQfjvVFZ5P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(X)\n",
        "Y_train_predicted = model.predict(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQHThFVMatxL",
        "outputId": "c6467a8d-3877-4bf6-ddd7-20171cb8b614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "601/601 [==============================] - 8s 12ms/step\n",
            "34117/34117 [==============================] - 381s 11ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_right_wrong(softmax_all, label):\n",
        "    mask_right = np.equal(np.argmax(softmax_all, axis=1), label)\n",
        "    mask_wrong = np.not_equal(np.argmax(softmax_all, axis=1), label)\n",
        "    right, wrong = softmax_all[mask_right], softmax_all[mask_wrong]\n",
        "    return right, wrong\n",
        "\n",
        "def right_wrong_distinction(model, test_images, test_labels):\n",
        "\n",
        "    softmax_all, _ = model.predict(test_images)\n",
        "    right_all, wrong_all = split_right_wrong(softmax_all, test_labels)\n",
        "\n",
        "    (s_prob_all, kl_all, mean_all, var_all) = entropy_stats(softmax_all)\n",
        "    (s_prob_right, kl_right, mean_right, var_right) = entropy_stats(right_all)\n",
        "    (s_prob_wrong, kl_wrong, mean_wrong, var_wrong) = entropy_stats(wrong_all)\n",
        "\n",
        "    correct_cases = np.equal(np.argmax(softmax_all, 1), test_labels)\n",
        "    accuracy = 100 * np.mean(np.float32(correct_cases))\n",
        "    err = 100 - accuracy\n",
        "\n",
        "    print(\"\\n[Error and Success Prediction]\")\n",
        "    print('\\nPrediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std)')\n",
        "    print(np.mean(s_prob_all), np.std(s_prob_all), '|',\n",
        "          np.mean(s_prob_right), np.std(s_prob_right), '|',\n",
        "          np.mean(s_prob_wrong), np.std(s_prob_wrong))\n",
        "\n",
        "    print('\\nSuccess base rate (%):', round(accuracy,2),\n",
        "          \"({}/{})\".format(len(right_all), len(softmax_all)))\n",
        "    print('KL[p||u]: Right/Wrong classification distinction')\n",
        "    print_curve_info(kl_right, kl_wrong)\n",
        "    print('Prediction Prob: Right/Wrong classification distinction')\n",
        "    print_curve_info(s_prob_right, s_prob_wrong)\n",
        "\n",
        "    print('\\nError base rate (%):', round(err,2),\n",
        "          \"({}/{})\".format(len(wrong_all), len(softmax_all)))\n",
        "    print('KL[p||u]: Right/Wrong classification distinction')\n",
        "    print_curve_info(-kl_right, -kl_wrong, True)\n",
        "    print('Prediction Prob: Right/Wrong classification distinction')\n",
        "    print_curve_info(-s_prob_right, -s_prob_wrong, True)\n",
        "\n",
        "    return (s_prob_right, s_prob_wrong, kl_right, kl_wrong)\n",
        "\n",
        "\n",
        "def in_out_distinction(model, model2, indist_images, outdist_images):\n",
        "    \n",
        "    softmax_indist, _ = model.predict(indist_images)\n",
        "    softmax_outdist, _ = model.predict(outdist_images)\n",
        "    \n",
        "    pseudo_indist, _ = model2.predict(indist_images)\n",
        "    pseudo_outdist, _ = model2.predict(outdist_images)\n",
        "    \n",
        "    (s_prob_in, _, _, _) = entropy_stats(softmax_indist)\n",
        "    (s_prob_out, _, _, _) = entropy_stats(softmax_outdist)\n",
        "    \n",
        "    (pseudo_prob_in, _, _, _) = entropy_stats(pseudo_indist)\n",
        "    (pseudo_prob_out, _, _, _) = entropy_stats(pseudo_outdist)\n",
        "    \n",
        "    print(\"\\n[In- and Out-of-Distribution Detection (MSP)]\\n\")\n",
        "    \n",
        "    print('In-dist max softmax distribution (mean, std):')\n",
        "    print(np.mean(s_prob_in), np.std(s_prob_in))\n",
        "    print('Out-of-dist max softmax distribution(mean, std):')\n",
        "    print(np.mean(s_prob_out), np.std(s_prob_out))\n",
        "    \n",
        "    print('\\nNormality Detection')\n",
        "    print('Normality base rate (%):',\n",
        "          round(100*indist_images.shape[0]/(outdist_images.shape[0]+indist_images.shape[0]),2))\n",
        "    print('Prediction Prob: Normality Detection')\n",
        "    print_curve_info(s_prob_in, s_prob_out)\n",
        "    \n",
        "    print('\\nAbnormality Detection')\n",
        "    print('Abnormality base rate (%):',\n",
        "          round(100*outdist_images.shape[0]/(outdist_images.shape[0]+indist_images.shape[0]),2))\n",
        "    print('Prediction Prob: Abnormality Detection')\n",
        "    print_curve_info(-s_prob_in, -s_prob_out, True)\n",
        "    \n",
        "    \n",
        "    print(\"\\n[In- and Out-of-Distribution Detection (OE)]\\n\")\n",
        "    \n",
        "    print('In-dist max softmax distribution (mean, std):')\n",
        "    print(np.mean(pseudo_prob_in), np.std(pseudo_prob_in))\n",
        "    print('Out-of-dist max softmax distribution(mean, std):')\n",
        "    print(np.mean(pseudo_prob_out), np.std(pseudo_prob_out))\n",
        "    \n",
        "    print('\\nNormality Detection')\n",
        "    print('Normality base rate (%):',\n",
        "          round(100*indist_images.shape[0]/(outdist_images.shape[0]+indist_images.shape[0]),2))\n",
        "    print('Prediction Prob: Normality Detection')\n",
        "    print_curve_info(pseudo_prob_in, pseudo_prob_out)\n",
        "    \n",
        "    print('\\nAbnormality Detection')\n",
        "    print('Abnormality base rate (%):',\n",
        "          round(100*outdist_images.shape[0]/(outdist_images.shape[0]+indist_images.shape[0]),2))\n",
        "    print('Prediction Prob: Abnormality Detection')\n",
        "    print_curve_info(-pseudo_prob_in, -pseudo_prob_out, True)\n",
        "    \n",
        "    return (s_prob_in, s_prob_out, pseudo_prob_in, pseudo_prob_out)\n",
        "\n",
        "\n",
        "\n",
        "def entropy_stats(softmax):\n",
        "    s_prob = np.amax(softmax, axis=1, keepdims=True)\n",
        "    kl_all = entropy_from_distribution(softmax, axis=1)\n",
        "    mean_all, var_all = np.mean(kl_all), np.var(kl_all)\n",
        "    return s_prob, kl_all, mean_all, var_all\n",
        "\n",
        "\n",
        "def entropy_from_distribution(p, axis):\n",
        "    return np.log(10.) + np.sum(p * np.log(np.abs(p) + 1e-11), axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def print_curve_info(safe, risky, inverse=False):\n",
        "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
        "    if inverse:\n",
        "        labels[safe.shape[0]:] += 1\n",
        "    else:\n",
        "        labels[:safe.shape[0]] += 1\n",
        "    examples = np.squeeze(np.vstack((safe, risky)))\n",
        "    print('AUPR (%):', round(100*sklearn.average_precision_score(labels, examples), 2))\n",
        "    print('AUROC (%):', round(100*sklearn.roc_auc_score(labels, examples), 2))\n"
      ],
      "metadata": {
        "id": "Wb0y66nya3NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVTd4l8fbwvc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}